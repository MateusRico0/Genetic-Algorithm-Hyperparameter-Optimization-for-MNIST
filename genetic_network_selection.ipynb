{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from collections import namedtuple\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperParams = Dict[str, any]\n",
    "\n",
    "SEARCH_SPACE = {\n",
    "    \"learning_rate\": (1e-4, 1e-1),\n",
    "    \"kernel_size\": [3, 5, 7],\n",
    "    \"batch_size\": [32, 64, 128],\n",
    "    \"optimizer\": [\"sgd\", \"adam\", \"rmsprop\"],\n",
    "    \"n_hidden_layers\": [1, 2, 3],\n",
    "    \"neurons\": [16, 32, 64, 128, 256],\n",
    "}\n",
    "\n",
    "Individual = namedtuple(\"Individual\", [\"hp\", \"fitness\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Fucntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp: HyperParams, input_shape=(28, 28, 1), n_classes=10):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    k = hp[\"kernel_size\"]\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            16, (k, k), activation=\"relu\", input_shape=input_shape, padding=\"same\"\n",
    "        )\n",
    "    )\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Hidden Layers\n",
    "    for neurons in hp[\"neurons\"]:\n",
    "        model.add(layers.Dense(neurons, activation=\"relu\"))\n",
    "    model.add(layers.Dense(n_classes, activation=\"softmax\"))\n",
    "\n",
    "    # Optimizer\n",
    "    lr = hp[\"learning_rate\"]\n",
    "    opt_name = hp[\"optimizer\"]\n",
    "    if opt_name == \"sgd\":\n",
    "        opt = optimizers.SGD(learning_rate=lr)\n",
    "    elif opt_name == \"adam\":\n",
    "        opt = optimizers.Adam(learning_rate=lr)\n",
    "    elif opt_name == \"rmsprop\":\n",
    "        opt = optimizers.RMSprop(learning_rate=lr)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid optimizer\")\n",
    "    model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_individual(hp: HyperParams, data, epochs=3, verbose=0) -> float:\n",
    "    x_train, y_train, x_val, y_val = data\n",
    "\n",
    "    model = build_model(hp)\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=hp[\"batch_size\"],\n",
    "        validation_data=(x_val, y_val),\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    val_acc = float(history.history[\"val_accuracy\"][-1])\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_individual() -> HyperParams:\n",
    "    hp = {}\n",
    "\n",
    "    lo, hi = SEARCH_SPACE[\"learning_rate\"]\n",
    "    hp[\"learning_rate\"] = 10 ** random.uniform(math.log10(lo), math.log10(hi))\n",
    "    hp[\"kernel_size\"] = random.choice(SEARCH_SPACE[\"kernel_size\"])\n",
    "    hp[\"batch_size\"] = random.choice(SEARCH_SPACE[\"batch_size\"])\n",
    "    hp[\"optimizer\"] = random.choice(SEARCH_SPACE[\"optimizer\"])\n",
    "    hp[\"n_hidden_layers\"] = random.choice(SEARCH_SPACE[\"n_hidden_layers\"])\n",
    "    hp[\"neurons\"] = [\n",
    "        random.choice(SEARCH_SPACE[\"neurons\"]) for _ in range(hp[\"n_hidden_layers\"])\n",
    "    ]\n",
    "\n",
    "    return hp\n",
    "\n",
    "def mutate(hp: HyperParams, mutation_rate=0.1) -> HyperParams:\n",
    "    new = copy.deepcopy(hp)\n",
    "\n",
    "    if random.random() < mutation_rate:\n",
    "        lo, hi = SEARCH_SPACE[\"learning_rate\"]\n",
    "        factor = 10 ** random.uniform(-0.5, 0.5)\n",
    "        new[\"learning_rate\"] = float(np.clip(new[\"learning_rate\"] * factor, lo, hi))\n",
    "    if random.random() < mutation_rate:\n",
    "        new[\"kernel_size\"] = random.choice(SEARCH_SPACE[\"kernel_size\"])\n",
    "    if random.random() < mutation_rate:\n",
    "        new[\"batch_size\"] = random.choice(SEARCH_SPACE[\"batch_size\"])\n",
    "    if random.random() < mutation_rate:\n",
    "        new[\"optimizer\"] = random.choice(SEARCH_SPACE[\"optimizer\"])\n",
    "    if random.random() < mutation_rate:\n",
    "        new[\"n_hidden_layers\"] = random.choice(SEARCH_SPACE[\"n_hidden_layers\"])\n",
    "        new[\"neurons\"] = [\n",
    "            random.choice(SEARCH_SPACE[\"neurons\"])\n",
    "            for _ in range(new[\"n_hidden_layers\"])\n",
    "        ]\n",
    "    else:\n",
    "        # Mutate neurons inside existing layers\n",
    "        for i in range(new[\"n_hidden_layers\"]):\n",
    "            if random.random() < mutation_rate:\n",
    "                new[\"neurons\"][i] = random.choice(SEARCH_SPACE[\"neurons\"])\n",
    "    return new\n",
    "\n",
    "def single_point_crossover(\n",
    "    hp1: HyperParams, hp2: HyperParams\n",
    ") -> Tuple[HyperParams, HyperParams]:\n",
    "    keys = [\n",
    "        \"learning_rate\",\n",
    "        \"kernel_size\",\n",
    "        \"batch_size\",\n",
    "        \"optimizer\",\n",
    "        \"n_hidden_layers\",\n",
    "        \"neurons\",\n",
    "    ]\n",
    "    point = random.randint(1, len(keys) - 1)\n",
    "    child1 = {}\n",
    "    child2 = {}\n",
    "    for i, k in enumerate(keys):\n",
    "        if i < point:\n",
    "            child1[k] = copy.deepcopy(hp1[k])\n",
    "            child2[k] = copy.deepcopy(hp2[k])\n",
    "        else:\n",
    "            child1[k] = copy.deepcopy(hp2[k])\n",
    "            child2[k] = copy.deepcopy(hp1[k])\n",
    "\n",
    "    for c in (child1, child2):\n",
    "        if len(c[\"neurons\"]) != c[\"n_hidden_layers\"]:\n",
    "            if len(c[\"neurons\"]) > c[\"n_hidden_layers\"]:\n",
    "                c[\"neurons\"] = c[\"neurons\"][: c[\"n_hidden_layers\"]]\n",
    "            else:\n",
    "                c[\"neurons\"].extend(\n",
    "                    [\n",
    "                        random.choice(SEARCH_SPACE[\"neurons\"])\n",
    "                        for _ in range(c[\"n_hidden_layers\"] - len(c[\"neurons\"]))\n",
    "                    ]\n",
    "                )\n",
    "    return child1, child2\n",
    "\n",
    "def roulette_wheel_selection(pop: List[Individual], k=1) -> List[HyperParams]:\n",
    "    fitnesses = np.array([max(0.0, ind.fitness) for ind in pop])\n",
    "    total = fitnesses.sum()\n",
    "    if total <= 0:\n",
    "        return [copy.deepcopy(random.choice(pop).hp) for _ in range(k)]\n",
    "    probs = fitnesses / total\n",
    "    chosen = np.random.choice(len(pop), size=k, p=probs)\n",
    "    return [copy.deepcopy(pop[i].hp) for i in chosen]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_standard_ga(data, pop_size=10, generations=5, epochs=3, mutation_rate=0.1):\n",
    "    pop = [Individual(sample_random_individual(), fitness=0.0) for _ in range(pop_size)]\n",
    "    history_best = []\n",
    "    all_pop_hist = [] # here\n",
    "\n",
    "    for i in range(len(pop)):\n",
    "        pop[i] = Individual(\n",
    "            pop[i].hp, evaluate_individual(pop[i].hp, data, epochs=epochs)\n",
    "        )\n",
    "    for gen in range(generations):\n",
    "        parents = roulette_wheel_selection(pop, k=pop_size)\n",
    "        offspring = []\n",
    "\n",
    "        for i in range(0, pop_size, 2):\n",
    "            p1 = parents[i % pop_size]\n",
    "            p2 = parents[(i + 1) % pop_size]\n",
    "            c1_hp, c2_hp = single_point_crossover(p1, p2)\n",
    "            offspring.append(Individual(mutate(c1_hp, mutation_rate), fitness=0.0))\n",
    "            if len(offspring) < pop_size:\n",
    "                offspring.append(Individual(mutate(c2_hp, mutation_rate), fitness=0.0))\n",
    "\n",
    "        for i in range(len(offspring)):\n",
    "            offspring[i] = Individual(\n",
    "                offspring[i].hp,\n",
    "                evaluate_individual(offspring[i].hp, data, epochs=epochs),\n",
    "            )\n",
    "\n",
    "        pop = offspring\n",
    "        best = max(pop, key=lambda ind: ind.fitness)\n",
    "        history_best.append(best.fitness)\n",
    "\n",
    "        all_pop_hist.append(pop) # here\n",
    "        print(\n",
    "            f\"[Standard GA] Gen {gen + 1}/{generations} best val_acc = {best.fitness:.4f}\"\n",
    "        )\n",
    "\n",
    "    return all_pop_hist, best, history_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latin Hypercube Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latin_hypercube_sampling(n_samples: int) -> List[HyperParams]:\n",
    "    rng = np.random.default_rng()\n",
    "    lr_lo, lr_hi = SEARCH_SPACE[\"learning_rate\"]\n",
    "    cut = np.linspace(0, 1, n_samples + 1)\n",
    "    u = rng.uniform(size=n_samples)\n",
    "\n",
    "    samples = []\n",
    "    for i in range(n_samples):\n",
    "        frac = cut[i] + u[i] * (cut[i + 1] - cut[i])\n",
    "        lr = 10 ** (math.log10(lr_lo) + frac * (math.log10(lr_hi) - math.log10(lr_lo)))\n",
    "        kernel = SEARCH_SPACE[\"kernel_size\"][i % len(SEARCH_SPACE[\"kernel_size\"])]\n",
    "        batch = SEARCH_SPACE[\"batch_size\"][(i + 1) % len(SEARCH_SPACE[\"batch_size\"])]\n",
    "        opt = SEARCH_SPACE[\"optimizer\"][(i + 2) % len(SEARCH_SPACE[\"optimizer\"])]\n",
    "        n_hidden = SEARCH_SPACE[\"n_hidden_layers\"][\n",
    "            i % len(SEARCH_SPACE[\"n_hidden_layers\"])\n",
    "        ]\n",
    "        neurons = [\n",
    "            SEARCH_SPACE[\"neurons\"][(i + j) % len(SEARCH_SPACE[\"neurons\"])]\n",
    "            for j in range(n_hidden)\n",
    "        ]\n",
    "\n",
    "        samples.append(\n",
    "            {\n",
    "                \"learning_rate\": lr,\n",
    "                \"kernel_size\": kernel,\n",
    "                \"batch_size\": batch,\n",
    "                \"optimizer\": opt,\n",
    "                \"n_hidden_layers\": n_hidden,\n",
    "                \"neurons\": neurons,\n",
    "            }\n",
    "        )\n",
    "    random.shuffle(samples)\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lhs_ga(data, pop_size=10, generations=5, epochs=3, mutation_rate=0.1):\n",
    "    initial_hps = latin_hypercube_sampling(pop_size)\n",
    "    pop = [Individual(hp, fitness=0.0) for hp in initial_hps]\n",
    "\n",
    "    for i in range(len(pop)):\n",
    "        pop[i] = Individual(\n",
    "            pop[i].hp, evaluate_individual(pop[i].hp, data, epochs=epochs)\n",
    "        )\n",
    "        \n",
    "    history_best = []\n",
    "    all_pop_hist = [] # here\n",
    "\n",
    "    for gen in range(generations):\n",
    "        parents = roulette_wheel_selection(pop, k=pop_size)\n",
    "        offspring = []\n",
    "        for i in range(0, pop_size, 2):\n",
    "            p1 = parents[i % pop_size]\n",
    "            p2 = parents[(i + 1) % pop_size]\n",
    "            c1_hp, c2_hp = single_point_crossover(p1, p2)\n",
    "            offspring.append(Individual(mutate(c1_hp, mutation_rate), fitness=0.0))\n",
    "            if len(offspring) < pop_size:\n",
    "                offspring.append(Individual(mutate(c2_hp, mutation_rate), fitness=0.0))\n",
    "        for i in range(len(offspring)):\n",
    "            offspring[i] = Individual(\n",
    "                offspring[i].hp,\n",
    "                evaluate_individual(offspring[i].hp, data, epochs=epochs),\n",
    "            )\n",
    "        pop = offspring\n",
    "        best = max(pop, key=lambda ind: ind.fitness)\n",
    "        history_best.append(best.fitness)\n",
    "        all_pop_hist.append(pop) # here\n",
    "\n",
    "        print(f\"[LHS GA] Gen {gen + 1}/{generations} best val_acc = {best.fitness:.4f}\")\n",
    "\n",
    "    return all_pop_hist, best, history_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Universal Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_universal_sampling(pop: List[Individual], k=1) -> List[HyperParams]:\n",
    "    fitnesses = np.array([max(0.0, ind.fitness) for ind in pop])\n",
    "    total = fitnesses.sum()\n",
    "    if total <= 0:\n",
    "        return [copy.deepcopy(random.choice(pop).hp) for _ in range(k)]\n",
    "    pointers = []\n",
    "    start = random.random() * (total / k)\n",
    "    step = total / k\n",
    "    pointers = [start + i * step for i in range(k)]\n",
    "    selected = []\n",
    "    cum = 0.0\n",
    "    idx = 0\n",
    "    for p in pointers:\n",
    "        while cum + fitnesses[idx] < p:\n",
    "            cum += fitnesses[idx]\n",
    "            idx += 1\n",
    "            if idx >= len(pop):\n",
    "                idx = len(pop) - 1\n",
    "                break\n",
    "        selected.append(copy.deepcopy(pop[idx].hp))\n",
    "\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sus_ga(data, pop_size=10, generations=5, epochs=3, mutation_rate=0.1):\n",
    "    pop = [Individual(sample_random_individual(), fitness=0.0) for _ in range(pop_size)]\n",
    "    for i in range(len(pop)):\n",
    "        pop[i] = Individual(\n",
    "            pop[i].hp, evaluate_individual(pop[i].hp, data, epochs=epochs)\n",
    "        )\n",
    "    history_best = []\n",
    "    all_pop_hist = [] # here\n",
    "\n",
    "    for gen in range(generations):\n",
    "        parents = stochastic_universal_sampling(pop, k=pop_size)\n",
    "        offspring = []\n",
    "        for i in range(0, pop_size, 2):\n",
    "            p1 = parents[i % pop_size]\n",
    "            p2 = parents[(i + 1) % pop_size]\n",
    "            c1_hp, c2_hp = single_point_crossover(p1, p2)\n",
    "            offspring.append(Individual(mutate(c1_hp, mutation_rate), fitness=0.0))\n",
    "            if len(offspring) < pop_size:\n",
    "                offspring.append(Individual(mutate(c2_hp, mutation_rate), fitness=0.0))\n",
    "        for i in range(len(offspring)):\n",
    "            offspring[i] = Individual(\n",
    "                offspring[i].hp,\n",
    "                evaluate_individual(offspring[i].hp, data, epochs=epochs),\n",
    "            )\n",
    "        pop = offspring\n",
    "        best = max(pop, key=lambda ind: ind.fitness)\n",
    "        history_best.append(best.fitness)\n",
    "        all_pop_hist.append(pop) # here\n",
    "\n",
    "        print(f\"[SUS GA] Gen {gen + 1}/{generations} best val_acc = {best.fitness:.4f}\")\n",
    "    return all_pop_hist, best, history_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform Crossover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_crossover(hp1: HyperParams, hp2: HyperParams, swap_prob=0.5):\n",
    "    child = {}\n",
    "    for key in hp1.keys():\n",
    "        if random.random() < swap_prob:\n",
    "            child[key] = copy.deepcopy(hp1[key])\n",
    "        else:\n",
    "            child[key] = copy.deepcopy(hp2[key])\n",
    "\n",
    "    if len(child[\"neurons\"]) != child[\"n_hidden_layers\"]:\n",
    "        if len(child[\"neurons\"]) > child[\"n_hidden_layers\"]:\n",
    "            child[\"neurons\"] = child[\"neurons\"][: child[\"n_hidden_layers\"]]\n",
    "        else:\n",
    "            child[\"neurons\"].extend(\n",
    "                [\n",
    "                    random.choice(SEARCH_SPACE[\"neurons\"])\n",
    "                    for _ in range(child[\"n_hidden_layers\"] - len(child[\"neurons\"]))\n",
    "                ]\n",
    "            )\n",
    "    return child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_uniform_crossover_ga(\n",
    "    data, pop_size=10, generations=5, epochs=3, mutation_rate=0.1\n",
    "):\n",
    "    pop = [Individual(sample_random_individual(), fitness=0.0) for _ in range(pop_size)]\n",
    "\n",
    "    for i in range(len(pop)):\n",
    "        pop[i] = Individual(\n",
    "            pop[i].hp, evaluate_individual(pop[i].hp, data, epochs=epochs)\n",
    "        )\n",
    "\n",
    "    history_best = []\n",
    "    all_pop_hist = [] # here\n",
    "\n",
    "    for gen in range(generations):\n",
    "        parents = roulette_wheel_selection(pop, k=pop_size)\n",
    "        offspring = []\n",
    "        for i in range(0, pop_size, 2):\n",
    "            p1 = parents[i % pop_size]\n",
    "            p2 = parents[(i + 1) % pop_size]\n",
    "            c1_hp = uniform_crossover(p1, p2, swap_prob=0.5)\n",
    "            c2_hp = uniform_crossover(p2, p1, swap_prob=0.5)\n",
    "            offspring.append(Individual(mutate(c1_hp, mutation_rate), fitness=0.0))\n",
    "            if len(offspring) < pop_size:\n",
    "                offspring.append(Individual(mutate(c2_hp, mutation_rate), fitness=0.0))\n",
    "\n",
    "        for i in range(len(offspring)):\n",
    "            offspring[i] = Individual(\n",
    "                offspring[i].hp,\n",
    "                evaluate_individual(offspring[i].hp, data, epochs=epochs),\n",
    "            )\n",
    "        pop = offspring\n",
    "        best = max(pop, key=lambda ind: ind.fitness)\n",
    "        history_best.append(best.fitness)\n",
    "        all_pop_hist.append(pop) # here\n",
    "\n",
    "        print(\n",
    "            f\"[Uniform Crossover GA] Gen {gen + 1}/{generations} best val_acc = {best.fitness:.4f}\"\n",
    "        )\n",
    "\n",
    "    return all_pop_hist, best, history_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_adaptive_mutation_ga(\n",
    "    data, pop_size=10, generations=5, epochs=3, base_mutation=0.1\n",
    "):\n",
    "    pop = [Individual(sample_random_individual(), fitness=0.0) for _ in range(pop_size)]\n",
    "    for i in range(len(pop)):\n",
    "        pop[i] = Individual(\n",
    "            pop[i].hp, evaluate_individual(pop[i].hp, data, epochs=epochs)\n",
    "        )\n",
    "    history_best = []\n",
    "    all_pop_hist = [] # here\n",
    "\n",
    "    mutation_rate = base_mutation\n",
    "    best_so_far = max(pop, key=lambda ind: ind.fitness)\n",
    "    stagnation = 0\n",
    "    for gen in range(generations):\n",
    "        parents = roulette_wheel_selection(pop, k=pop_size)\n",
    "        offspring = []\n",
    "        for i in range(0, pop_size, 2):\n",
    "            p1 = parents[i % pop_size]\n",
    "            p2 = parents[(i + 1) % pop_size]\n",
    "            c1_hp, c2_hp = single_point_crossover(p1, p2)\n",
    "            offspring.append(Individual(mutate(c1_hp, mutation_rate), fitness=0.0))\n",
    "            if len(offspring) < pop_size:\n",
    "                offspring.append(Individual(mutate(c2_hp, mutation_rate), fitness=0.0))\n",
    "        for i in range(len(offspring)):\n",
    "            offspring[i] = Individual(\n",
    "                offspring[i].hp,\n",
    "                evaluate_individual(offspring[i].hp, data, epochs=epochs),\n",
    "            )\n",
    "        pop = offspring\n",
    "        current_best = max(pop, key=lambda ind: ind.fitness)\n",
    "        history_best.append(current_best.fitness)\n",
    "        all_pop_hist.append(pop) # here\n",
    "        \n",
    "        # adapt mutation\n",
    "        if current_best.fitness <= best_so_far.fitness + 1e-6:\n",
    "            stagnation += 1\n",
    "            mutation_rate = min(0.5, base_mutation * (1 + stagnation * 0.5))  # increase\n",
    "        else:\n",
    "            stagnation = 0\n",
    "            mutation_rate = max(0.01, base_mutation * 0.5)  # reduce\n",
    "            best_so_far = current_best\n",
    "        print(\n",
    "            f\"[Adaptive Mut GA] Gen {gen + 1}/{generations} best val_acc = {current_best.fitness:.4f} mutation_rate={mutation_rate:.3f}\"\n",
    "        )\n",
    "    return all_pop_hist, best_so_far, history_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steady-State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tournament_selection(pop: List[Individual], k=1, tourn_size=3):\n",
    "    selected = []\n",
    "    for _ in range(k):\n",
    "        contenders = random.sample(pop, min(tourn_size, len(pop)))\n",
    "        winner = max(contenders, key=lambda ind: ind.fitness)\n",
    "        selected.append(copy.deepcopy(winner.hp))\n",
    "\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_steady_state_ga(\n",
    "    data, pop_size=10, generations=20, epochs=3, mutation_rate=0.1, replace_k=2\n",
    "):\n",
    "    pop = [Individual(sample_random_individual(), fitness=0.0) for _ in range(pop_size)]\n",
    "    for i in range(len(pop)):\n",
    "        pop[i] = Individual(\n",
    "            pop[i].hp, evaluate_individual(pop[i].hp, data, epochs=epochs)\n",
    "        )\n",
    "\n",
    "    history_best = []\n",
    "    all_pop_hist = [] # here\n",
    "\n",
    "    for gen in range(generations):\n",
    "        parents = tournament_selection(pop, k=replace_k * 2, tourn_size=3)\n",
    "\n",
    "        offspring = []\n",
    "        for i in range(0, len(parents), 2):\n",
    "            p1 = parents[i]\n",
    "            p2 = parents[(i + 1) % len(parents)]\n",
    "            c1_hp, c2_hp = single_point_crossover(p1, p2)\n",
    "            offspring.append(Individual(mutate(c1_hp, mutation_rate), fitness=0.0))\n",
    "            if len(offspring) < replace_k:\n",
    "                offspring.append(Individual(mutate(c2_hp, mutation_rate), fitness=0.0))\n",
    "\n",
    "        for i in range(len(offspring)):\n",
    "            offspring[i] = Individual(\n",
    "                offspring[i].hp,\n",
    "                evaluate_individual(offspring[i].hp, data, epochs=epochs),\n",
    "            )\n",
    "\n",
    "        pop_sorted = sorted(pop, key=lambda ind: ind.fitness)\n",
    "        for i in range(len(offspring)):\n",
    "            pop_sorted[i] = offspring[i]\n",
    "\n",
    "        pop = pop_sorted\n",
    "        best = max(pop, key=lambda ind: ind.fitness)\n",
    "        history_best.append(best.fitness)\n",
    "        all_pop_hist.append(pop) # here\n",
    "\n",
    "        print(\n",
    "            f\"[Steady-State GA] Gen {gen + 1}/{generations} best val_acc = {best.fitness:.4f}\"\n",
    "        )\n",
    "\n",
    "    return all_pop_hist, best, history_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_fraction=1.0, val_fraction=0.2):\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    x = np.concatenate([x_train, x_test], axis=0)\n",
    "    y = np.concatenate([y_train, y_test], axis=0)\n",
    "\n",
    "    x = x.astype(\"float32\") / 255.0\n",
    "    x = np.expand_dims(x, -1)\n",
    "    y = to_categorical(y, 10)\n",
    "\n",
    "    idx = np.arange(len(x))\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    x = x[idx]\n",
    "    y = y[idx]\n",
    "\n",
    "    # subset\n",
    "    if data_fraction < 1.0:\n",
    "        n = int(len(x) * data_fraction)\n",
    "        x = x[:n]\n",
    "        y = y[:n]\n",
    "\n",
    "    n_val = int(len(x) * val_fraction)\n",
    "    x_val = x[:n_val]\n",
    "    y_val = y[:n_val]\n",
    "    x_train = x[n_val:]\n",
    "    y_train = y[n_val:]\n",
    "\n",
    "    return (x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "POP_SIZE = 3\n",
    "GENERATIONS = 1\n",
    "\n",
    "EPOCHS = 1  # Model epochs\n",
    "DATA_FRACTION = 0.1  # MNIST data Fraction\n",
    "\n",
    "data = prepare_data(data_fraction=DATA_FRACTION)\n",
    "\n",
    "results = {}\n",
    "histories = {}\n",
    "\n",
    "start_all = time.time()\n",
    "\n",
    "print(\"\\n=== Standard GA ===\")\n",
    "pop_hist_ga, best_std, hist_std = run_standard_ga(\n",
    "    data,\n",
    "    pop_size=POP_SIZE,\n",
    "    generations=GENERATIONS,\n",
    "    epochs=EPOCHS,\n",
    "    mutation_rate=0.12,\n",
    ")\n",
    "results[\"Standard\"] = best_std\n",
    "histories[\"Standard\"] = hist_std\n",
    "\n",
    "\n",
    "print(\"\\n=== LHS GA ===\")\n",
    "pop_hist_lhs, best_lhs, hist_lhs = run_lhs_ga(\n",
    "    data,\n",
    "    pop_size=POP_SIZE,\n",
    "    generations=GENERATIONS,\n",
    "    epochs=EPOCHS,\n",
    "    mutation_rate=0.12,\n",
    ")\n",
    "results[\"LHS\"] = best_lhs\n",
    "histories[\"LHS\"] = hist_lhs\n",
    "\n",
    "print(\"\\n=== Running SUS GA ===\")\n",
    "pop_hist_sus, best_sus, hist_sus = run_sus_ga(\n",
    "    data,\n",
    "    pop_size=POP_SIZE,\n",
    "    generations=GENERATIONS,\n",
    "    epochs=EPOCHS,\n",
    "    mutation_rate=0.12,\n",
    ")\n",
    "results[\"SUS\"] = best_sus\n",
    "histories[\"SUS\"] = hist_sus\n",
    "\n",
    "print(\"\\n=== Uniform Crossover GA ===\")\n",
    "pop_hist_uni, best_uni, hist_uni = run_uniform_crossover_ga(\n",
    "    data,\n",
    "    pop_size=POP_SIZE,\n",
    "    generations=GENERATIONS,\n",
    "    epochs=EPOCHS,\n",
    "    mutation_rate=0.12,\n",
    ")\n",
    "results[\"UniformCrossover\"] = best_uni\n",
    "histories[\"UniformCrossover\"] = hist_uni\n",
    "\n",
    "print(\"\\n=== Adaptive Mutation GA ===\")\n",
    "pop_hist_adpt, best_adapt, hist_adapt = run_adaptive_mutation_ga(\n",
    "    data,\n",
    "    pop_size=POP_SIZE,\n",
    "    generations=GENERATIONS,\n",
    "    epochs=EPOCHS,\n",
    "    base_mutation=0.12,\n",
    ")\n",
    "results[\"AdaptiveMutation\"] = best_adapt\n",
    "histories[\"AdaptiveMutation\"] = hist_adapt\n",
    "\n",
    "print(\"\\n=== Steady-State GA ===\")\n",
    "pop_hist_ss, best_ss, hist_ss = run_steady_state_ga(\n",
    "    data,\n",
    "    pop_size=POP_SIZE,\n",
    "    generations=GENERATIONS,\n",
    "    epochs=EPOCHS,\n",
    "    mutation_rate=0.12,\n",
    "    replace_k=5,\n",
    ")\n",
    "results[\"SteadyState\"] = best_ss\n",
    "histories[\"SteadyState\"] = hist_ss\n",
    "\n",
    "total_time = time.time() - start_all\n",
    "print(f\"\\nAll runs took {total_time:.1f} seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
